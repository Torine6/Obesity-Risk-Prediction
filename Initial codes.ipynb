{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "baseline_test_preds = baseline_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and precision for train and test sets\n",
    "def evaluate_model_performance(train_true, train_pred, test_true, test_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a model on both training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_true (array-like): True labels of the training set.\n",
    "    - train_pred (array-like): Predicted labels on the training set.\n",
    "    - test_true (array-like): True labels of the testing set.\n",
    "    - test_pred (array-like): Predicted labels on the testing set.\n",
    "\n",
    "    Prints and returns:\n",
    "    - Train Set Metrics:\n",
    "        - Precision score, accuracy score, recall score, F1 score and classification report on the training set.\n",
    "    - Test Set Metrics:\n",
    "        - Precision, accuracy , recall , F1 score and classification report on the testing set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train Set Precision\n",
    "    train_precision = precision_score(train_true, train_pred, average=\"weighted\")\n",
    "    print(f\"Train Precision: {train_precision}\")\n",
    "    \n",
    "     # Test Set Precision\n",
    "    test_precision = precision_score(test_true, test_pred, average=\"weighted\")\n",
    "    print(f\"Test Precision: {test_precision}\")\n",
    "    \n",
    "    print(\"------------\")\n",
    "    \n",
    "    # Train Set Accuracy\n",
    "    train_accuracy = accuracy_score(train_true, train_pred)\n",
    "    print(f\"Train Accuracy: {train_accuracy}\")\n",
    "    \n",
    "    # Test Set Accuracy\n",
    "    test_accuracy = accuracy_score(test_true, test_pred)\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    print(\"------------\")\n",
    "\n",
    "    # Train Set Recall\n",
    "    train_recall = recall_score(train_true, train_pred, average=\"weighted\")\n",
    "    print(f\"Train Recall: {train_recall}\")\n",
    "    \n",
    "    # Test Set Recall\n",
    "    test_recall = recall_score(test_true, test_pred, average=\"weighted\")\n",
    "    print(f\"Test Recall: {test_recall}\")\n",
    "    \n",
    "    print(\"------------\")    \n",
    "\n",
    "    # Train Set F1-Score\n",
    "    train_f1 = f1_score(train_true, train_pred, average=\"weighted\")\n",
    "    print(f\"Train F1 Score: {train_f1}\")\n",
    "    \n",
    "    # Test Set F1-Score\n",
    "    test_f1 = f1_score(test_true, test_pred, average=\"weighted\")\n",
    "    print(f\"Test F1 Score: {test_f1}\") \n",
    "\n",
    "# Evaluate our baseline model performance\n",
    "evaluate_model_performance(y_train, baseline_train_preds, y_test, baseline_test_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SMOTE object\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "\n",
    "# Fit and transform the training data using SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Train a logistic regression model on the resampled data\n",
    "logreg_model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_model1.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Obtain predictions on the training set\n",
    "train_preds_smote = logreg_model1.predict(X_resampled)\n",
    "\n",
    "# Obtain predictions on the test set\n",
    "test_preds_smote = logreg_model1.predict(X_test_scaled)\n",
    "\n",
    "# Use the evaluate_model_performance function\n",
    "evaluate_model_performance(y_resampled, train_preds_smote, y_test, test_preds_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a Random Forest Classifier\n",
    "RForest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "RForest.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "RForest_train_preds = RForest.predict(X_train_scaled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "RForest_test_preds = RForest.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the Random Forest model\n",
    "evaluate_model_performance(y_train, RForest_train_preds, y_test, RForest_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a threshold for feature importance\n",
    "threshold = 0.02 \n",
    "\n",
    "# Identify features to keep based on the threshold\n",
    "selected_features = feature_names[feature_importances > threshold]\n",
    "\n",
    "# Subset your training data to keep only the selected features\n",
    "X_train_selected = X_train_scaled[selected_features]\n",
    "\n",
    "# Train your model on the subset of features using the training set\n",
    "RForest.fit(X_train_selected, y_train)\n",
    "\n",
    "# Use the same selected features on the test set\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_preds_selected = RForest.predict(X_train_selected)\n",
    "\n",
    "# Evaluate model performance using the function\n",
    "evaluate_train_precision(y_train, train_preds_selected) #**not original one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
